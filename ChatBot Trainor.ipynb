{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\grant\\anaconda3\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tensorflow) (0.30.0)\n",
      "Requirement already satisfied: tensorboard<1.8.0,>=1.7.0 in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tensorflow) (1.7.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tensorflow) (0.6.2)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tensorflow) (3.5.2.post1)\n",
      "Requirement already satisfied: bleach==1.5.0 in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow) (1.5.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow) (2.6.11)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: html5lib==0.9999999 in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow) (0.9999999)\n",
      "Requirement already satisfied: setuptools in c:\\users\\grant\\anaconda3\\lib\\site-packages (from protobuf>=3.4.0->tensorflow) (38.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "notebook 5.4.0 requires ipykernel, which is not installed.\n",
      "jupyter 1.0.0 requires ipykernel, which is not installed.\n",
      "jupyter-console 5.2.0 requires ipykernel, which is not installed.\n",
      "ipywidgets 7.1.1 requires ipykernel>=4.5.1, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tflearn/tflearn.git\n",
      "  Cloning https://github.com/tflearn/tflearn.git to c:\\users\\grant\\appdata\\local\\temp\\pip-req-build-4v9xr29b\n",
      "Requirement already satisfied (use --upgrade to upgrade): tflearn==0.3.2 from git+https://github.com/tflearn/tflearn.git in c:\\users\\grant\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: numpy in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tflearn==0.3.2) (1.14.0)\n",
      "Requirement already satisfied: six in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tflearn==0.3.2) (1.11.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\grant\\anaconda3\\lib\\site-packages (from tflearn==0.3.2) (5.0.0)\n",
      "Building wheels for collected packages: tflearn\n",
      "  Running setup.py bdist_wheel for tflearn: started\n",
      "  Running setup.py bdist_wheel for tflearn: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\grant\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-x0gxk87i\\wheels\\5a\\18\\2a\\c62b9937b37223da935fb6b2965f02fbc45691e460a08b91b4\n",
      "Successfully built tflearn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "notebook 5.4.0 requires ipykernel, which is not installed.\n",
      "jupyter 1.0.0 requires ipykernel, which is not installed.\n",
      "jupyter-console 5.2.0 requires ipykernel, which is not installed.\n",
      "ipywidgets 7.1.1 requires ipykernel>=4.5.1, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/tflearn/tflearn.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Do this in a separate python interpreter session, since you only have to do it once\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Do this in your ipython notebook or analysis script\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# things we need for NLP\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# things we need for Tensorflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'greating', 'patterns': ['Hello', 'How are you doing?', 'Good Morning', 'Good Afternoon'], 'responses': ['Hello!', 'Good to see you!'], 'context_set': ''}, {'tag': 'types', 'patterns': ['What types of damages are available for this case?', 'What types of damages are available to the plaintiff?', 'What types of damages are available in this case?'], 'responses': ['Typically damages are calculate in general and special in California', 'California provides special and general damages to plaintiffs']}, {'tag': 'general', 'patterns': ['Can you explain general damages?', 'Explain general damages to the jury?', 'What is the meaning of general damages?'], 'responses': ['General damages are non-economic damages suffered by the plaintiff', 'General damages are defined as non-economic damages']}, {'tag': 'special', 'patterns': ['Can you explain special damages?', 'Explain special damages to the jury?', 'What is the meaning of special damages?'], 'responses': ['Special damages are economic damages suffered by the plaintiff', 'Special damages are defined as economic damages']}, {'tag': 'noneconomic', 'patterns': ['What are noneconomic damages?', 'What do noneconomic damages include?', 'What is the meaning of noneconomic damages?'], 'responses': ['Noneconomic damages are intangible losses and can incldue pain and suffering, mental anguish and loss of consortium', 'Some exaples of noneconomic damages are include pain and suffering, mental anguish and loss of consortium, but include all intangible losses']}, {'tag': 'economic', 'patterns': ['Can are economic damages?', 'What do economic damages include?', 'What is the meaning of economic damages?'], 'responses': ['Economic damages are tangible losses and can include costs of nursing care, physical rehabilitation and vocational rehabilitation?', 'Some examples of economic damages are costs of nursing care, physical rehabilitation and vocational rehabilitation, but can include all tangible losses']}, {'tag': 'single', 'patterns': ['What is the cost of a single wide trailer?', 'What is the price of a single wide trailer?', 'How much do you think a single wide trailer costs?'], 'responses': ['A single wide trailor costs around forty thousand dollars new, and ten to twenty thousand dollars used.']}, {'tag': 'double', 'patterns': ['What is the cost of a double wide trailer?', 'What is the price of a double wide trailer?', 'How much do you think a double wide trailer costs?'], 'responses': ['A double wide trailor costs around seventy five thousand dollars new, and twenty to fifty thousand dollars used.']}, {'tag': 'construction', 'patterns': ['What do you think the cost of rebuilding the trailer would cost?', 'What are the construction costs required to rebuild this trailer?'], 'responses': ['Construction costs for a modular home range from seventeen thousand dollars at the low end up to fifty thousand dollars.']}, {'tag': 'pain', 'patterns': ['How would you calculate pain and suffering?', 'What is the calculation for pain and suffering?'], 'responses': ['Usually pain and suffering is calculated multiplying the medical bills by a multiplier between one and five.']}, {'tag': 'medical', 'patterns': ['What were the total medical costs?'], 'responses': ['The plaintiff suffered one thousand dollars in medical bills.']}, {'tag': 'multiplier', 'patterns': ['What should be the multiplier in this case?', 'What do you believe the multiplier should be?'], 'responses': ['The multiplier should be between one and two considering the minor injuries']}, {'tag': 'calculated', 'patterns': ['What are the calculated pain and suffering damages that should be awarded to the plaintiff?', 'What do you calculate the pain and suffering to be awared to the plaintiff?'], 'responses': ['Two thousand dollars.'], 'Context_Set': 'damages'}, {'tag': 'total', 'patterns': ['What are the total damages in this case?', 'What are your calculations for total damages in this case?'], 'responses': ['The total damages in this case should be around twenty and fifty thousand dollars.'], 'context_filter': 'damages'}]}\n"
     ]
    }
   ],
   "source": [
    "# import our chat-bot intents file\n",
    "import json\n",
    "\n",
    "f = open('c:/Users/grant/desktop/AiLawMiniCourseHW/book.txt', 'r')\n",
    "s = f.read()\n",
    "\n",
    "intents = json.loads(s)\n",
    "print(intents)\n",
    "\n",
    "#with open('c:/Users/grant/desktop/AiLawMiniCourseHW/book.txt', 'r') as book_data:\n",
    "#    json_data = json.load(book_data)\n",
    "#    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 documents\n",
      "14 classes ['calculated', 'construction', 'double', 'economic', 'general', 'greating', 'medical', 'multiplier', 'noneconomic', 'pain', 'single', 'special', 'total', 'types']\n",
      "59 unique stemmed words ['a', 'afternoon', 'and', 'ar', 'avail', 'aw', 'award', 'be', 'believ', 'calc', 'can', 'cas', 'construct', 'cost', 'dam', 'do', 'doing', 'doubl', 'econom', 'explain', 'for', 'gen', 'good', 'hello', 'how', 'in', 'includ', 'is', 'jury', 'mean', 'med', 'morn', 'much', 'multiply', 'noneconom', 'of', 'pain', 'plaintiff', 'pric', 'rebuild', 'requir', 'should', 'singl', 'spec', 'suff', 'that', 'the', 'thi', 'think', 'to', 'tot', 'trail', 'typ', 'wer', 'what', 'wid', 'would', 'yo', 'you']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        # add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4999  | total loss: \u001b[1m\u001b[32m0.07850\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1000 | loss: 0.07850 - acc: 0.9997 -- iter: 32/36\n",
      "Training Step: 5000  | total loss: \u001b[1m\u001b[32m0.07657\u001b[0m\u001b[0m | time: 0.015s\n",
      "| Adam | epoch: 1000 | loss: 0.07657 - acc: 0.9997 -- iter: 36/36\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\grant\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "['calculated', 'construction', 'double', 'economic', 'general', 'medical', 'multiplier', 'noneconomic', 'pain', 'single', 'special', 'total', 'types']\n"
     ]
    }
   ],
   "source": [
    "p = bow(\"is your shop open today?\", words)\n",
    "print (p)\n",
    "print (classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01620386 0.14024475 0.18542093 0.02958297 0.01148112 0.04139716\n",
      "  0.19776902 0.03262744 0.05329175 0.14057268 0.05986594 0.07605327\n",
      "  0.0154891 ]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of our data structures\n",
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
